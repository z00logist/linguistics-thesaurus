# Neural Networks in NLP

Neural networks are computational models inspired by how the human brain works. They have become a powerful tool in NLP, allowing models to recognize and learn complex language patterns.

## Types of Neural Networks

- Feedforward neural networks are the simplest type, where data moves in one direction.
- Recurrent neural networks (RNNs) are designed for sequences, retaining information about previous inputs to handle context.
- Long Short-Term Memory networks (LSTMs) are a kind of RNN capable of learning long-term dependencies in data.
- Convolutional neural networks (CNNs) capture local patterns effectively, making them useful for tasks like text classification.
- Transformer networks use attention mechanisms to manage dependencies, eliminating the need for sequential processing.

## Related Topics

- [Word Embeddings](Word-Embeddings/README.md)
- [Attention Mechanisms](Attention-Mechanisms.md)
- [Transformers](Transformers.md)

## Additional Resources

- [Neural Networks for NLP - Coursera](https://www.coursera.org/learn/deep-neural-networks-with-pytorch)
- [Deep Learning Book](https://www.deeplearningbook.org/) by Goodfellow, Bengio, and Courville

## Sources

- Goldberg, Y. (2017). *Neural Network Methods for Natural Language Processing*. Morgan & Claypool Publishers.
- Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*. MIT Press.

---

[Back to Computational Linguistics](../README.md)
