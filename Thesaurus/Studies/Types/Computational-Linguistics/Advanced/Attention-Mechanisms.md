# Attention Mechanisms

Attention mechanisms help models focus on relevant parts of the input while producing each part of the output. This is especially valuable for tasks that rely heavily on context.

## Key Concepts

- Alignment measures how input and output elements relate to each other.
- A context vector represents a weighted combination of input features, with weights determined by attention scores.
- Self-attention works within a single sequence, letting the model assign importance to different parts of that sequence.

## Multihead Attention

![Attention](../../../../assets/multi-head-attention.png)

## Explore Further

- [Neural Networks in NLP](Neural-Networks-in-NLP.md)
- [Transformers](../Language-Models/Transformers.md)
- [Large Language Models](../Language-Models/Large-Language-Models.md)

## Additional Reading

- [Attention Mechanisms in NLP](https://medium.com/syncedreview/a-brief-overview-of-attention-mechanism-13c578ba9129)
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)

## Sources

- Bahdanau, D., Cho, K., & Bengio, Y. (2015). "Neural Machine Translation by Jointly Learning to Align and Translate." *Proceedings of the International Conference on Learning Representations*.
- Vaswani, A., et al. (2017). "Attention is All You Need." *Advances in Neural Information Processing Systems*, 5998â€“6008.

---

[Back to Computational Linguistics](../README.md)
