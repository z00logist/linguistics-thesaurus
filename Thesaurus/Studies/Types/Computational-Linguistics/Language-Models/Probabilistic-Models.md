# Probabilistic Models

Probabilistic models are used in computational linguistics to apply probability theory to linguistic phenomena. They are essential for handling the uncertainty and variability present in language.

## Types of Probabilistic Models

- Hidden Markov Models (HMMs) are applied to sequence labeling tasks, such as part-of-speech tagging.
- Bayesian Networks represent the probabilistic relationships between different variables.
- Conditional Random Fields (CRFs) are used for structured prediction and improve over HMMs by taking future states into account.

## Important Concepts

- Maximum Likelihood Estimation (MLE) helps estimate parameters that maximize the likelihood of observed data.
- Bayesian Inference updates beliefs or probabilities when new evidence is introduced.

## Related Topics

- [N-gram Models](N-gram-Models.md)
- [Hidden Markov Models](Hidden-Markov-Models.md)

## Additional Resources

- [Probabilistic Models in NLP on Wikipedia](https://en.wikipedia.org/wiki/Statistical_natural_language_processing)
- [Introduction to Probabilistic Models](https://web.stanford.edu/~jurafsky/slp3/5.pdf) (Stanford)

## Sources

- Manning, C. D., & Sch√ºtze, H. (1999). *Foundations of Statistical Natural Language Processing*. MIT Press.
- Murphy, K. P. (2012). *Machine Learning: A Probabilistic Perspective*. MIT Press.

---

[Back to Computational Linguistics](../README.md)
