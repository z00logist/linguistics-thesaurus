# Word Embeddings

Word embeddings are vector representations of words that capture their meanings, semantic relationships, and syntactic properties in a continuous vector space.

## Contents

- [Word Embeddings](Word-Embeddings.md)
- [Bag of Words](Bag-ofWords.md)
- [Word2Vec](Word2Vec.md)
- [GloVe](GloVe.md)
- [FastText](FastText.md)

---

[Back to Computational Linguistics](../README.md)
