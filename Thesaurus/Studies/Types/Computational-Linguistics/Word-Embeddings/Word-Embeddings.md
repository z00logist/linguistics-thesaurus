# Word Embeddings

Word embeddings are techniques that map words or phrases from a vocabulary to numerical vector representations. These vectors are designed to capture the semantic relationships between words.

## Key Concepts

- Distributed representation means words are embedded in a continuous vector space.
- Semantic similarity ensures that similar words have vectors that are close to each other.
- Dimensionality reduction represents high-dimensional language data in a simpler, lower-dimensional form.

## Common Methods

- Predictive models, like Word2Vec, learn embeddings by predicting context words.
- Count-based models, such as GloVe, rely on word co-occurrence statistics to build word vectors.

## Related Topics

- [Word2Vec](Word2Vec.md)
- [GloVe](GloVe.md)

## Additional Resources

- [Word Embeddings - Wikipedia](https://en.wikipedia.org/wiki/Word_embedding)
- [TensorFlow Word Embeddings Tutorial](https://www.tensorflow.org/tutorials/text/word_embeddings)

## Sources

- Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). "Efficient Estimation of Word Representations in Vector Space." *arXiv preprint arXiv:1301.3781*.
- Pennington, J., Socher, R., & Manning, C. D. (2014). "GloVe: Global Vectors for Word Representation." *Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing*, 1532â€“1543.

---

[Back to Computational Linguistics](../README.md)