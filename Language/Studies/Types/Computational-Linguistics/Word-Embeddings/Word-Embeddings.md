# Word Embeddings

Word embeddings map words or phrases from a vocabulary to numerical vector representations. These vectors capture semantic similarities between words.

## Concepts

- **Distributed Representation**: Words are represented in a continuous vector space.
- **Semantic Similarity**: Similar words have similar vectors.
- **Dimensionality Reduction**: High-dimensional data is represented in a lower-dimensional space.

## Methods

- **Predictive Models**: Learn embeddings by predicting context words (e.g., Word2Vec).
- **Count-Based Models**: Use word co-occurrence statistics (e.g., GloVe).

## Applications

- **Semantic Analysis**: Measuring word similarity.
- **Machine Translation**: Mapping words across languages.
- **Information Retrieval**: Enhancing search algorithms.

## Related Topics

- [Word2Vec](Word2Vec.md)
- [GloVe](GloVe.md)
- [Neural Networks in NLP](../Neural-Networks-in-NLP.md)

## External Links

- [Word Embeddings - Wikipedia](https://en.wikipedia.org/wiki/Word_embedding)
- [TensorFlow Word Embeddings Tutorial](https://www.tensorflow.org/tutorials/text/word_embeddings)

## References

- Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). "Efficient Estimation of Word Representations in Vector Space." *arXiv preprint arXiv:1301.3781*.
- Pennington, J., Socher, R., & Manning, C. D. (2014). "GloVe: Global Vectors for Word Representation." *Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing*, 1532â€“1543.

---

[Back to Word Embeddings](README.md)
