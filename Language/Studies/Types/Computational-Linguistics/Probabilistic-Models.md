# Probabilistic Models

Probabilistic models in computational linguistics use probability theory to model linguistic phenomena. They help in dealing with uncertainty and variability in language.

## Types of Probabilistic Models

- **Hidden Markov Models (HMMs)**: Used for sequence labeling tasks like part-of-speech tagging.
- **Bayesian Networks**: Represent probabilistic relationships among variables.
- **Conditional Random Fields (CRFs)**: Used for structured prediction, improving over HMMs by considering future states.

## Applications

- **Part-of-Speech Tagging**: Assigning grammatical categories to words.
- **Speech Recognition**: Modeling the probability of phoneme sequences.
- **Machine Translation**: Estimating the probability of translation pairs.

## Concepts

- **Maximum Likelihood Estimation (MLE)**: Estimating parameters that maximize the likelihood of observed data.
- **Bayesian Inference**: Updating beliefs based on evidence.

## Related Topics

- [N-gram Models](N-gram-Models.md)
- [Hidden Markov Models](Hidden-Markov-Models.md)
- [Statistical NLP](Statistical-NLP.md)

## External Links

- [Probabilistic Models in NLP - Wikipedia](https://en.wikipedia.org/wiki/Statistical_natural_language_processing)
- [Introduction to Probabilistic Models](https://web.stanford.edu/~jurafsky/slp3/5.pdf) (Stanford)

## References

- Manning, C. D., & Sch√ºtze, H. (1999). *Foundations of Statistical Natural Language Processing*. MIT Press.
- Murphy, K. P. (2012). *Machine Learning: A Probabilistic Perspective*. MIT Press.

---

[Back to Computational Linguistics](README.md)
